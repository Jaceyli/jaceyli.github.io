<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="baidu-site-verification" content="E91H9LxOVM" />
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="keyword"  content="">
    <link rel="shortcut icon" href="/img/favicon.png">
    <!-- Place this tag in your head or just before your close body tag. -->
    <script async defer src="https://buttons.github.io/buttons.js"></script>
    <title>
        
          Bayesian Classifiers - Jingxuan&#39;s blog
        
    </title>

    <link rel="canonical" href="https://jingxuan.li/2018/06/08/Bayesian-Classifiers/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS --> 
    <link rel="stylesheet" href="/css/beantech.min.css">
    
    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css">

    <link rel="stylesheet" href="/css/widget.css">

    <link rel="stylesheet" href="/css/rocket.css">

    <link rel="stylesheet" href="/css/signature.css">

    <link rel="stylesheet" href="/css/toc.css">

    <!-- Custom Fonts -->
    <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <!--<link href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">-->


    <!-- Hux Delete, sad but pending in China
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/
    css'>
    -->


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116294246-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-116294246-1');
    </script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">
	<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->
<style type="text/css">
    header.intro-header{
        
            background-image: url('bg.jpg')
            /*post*/
        
    }
    
</style>

<header class="intro-header" >
    <!-- Signature -->
    <div id="signature">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                
                    <div class="post-heading">
                        <div class="tags">
                            
                              <a class="tag" href="/tags/#COMP9318" title="COMP9318">COMP9318</a>
                            
                              <a class="tag" href="/tags/#Data Mining" title="Data Mining">Data Mining</a>
                            
                              <a class="tag" href="/tags/#Classification" title="Classification">Classification</a>
                            
                        </div>
                        <h1>Bayesian Classifiers</h1>
                        <h2 class="subheading">COMP9318 Week 7 @18S1 UNSW</h2>
                        <span class="meta">
                            Posted by Jingxuan on
                            2018-06-08
                        </span>
                    </div>
                


                </div>
            </div>
        </div>
    </div>
</header>

	
    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Jingxuan&#39;s Blog</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>

                    

                        
                    

                        
                        <li>
                            <a href="/archive/">Archives</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                    

                        
                        <li>
                            <a href="/tags/">Tags</a>
                        </li>
                        
                    
                    
                </ul>
            </div>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>


    <!-- Main Content -->
    <!-- Modify by Yu-Hsuan Yen -->

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

            <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <blockquote>
<p>Abstract:</p>
<ul>
<li>Bayesian Theorem</li>
<li>Naïve Bayes Classifier</li>
<li>Smoothing</li>
</ul>
</blockquote>
<style>
table{
    margin: 0 auto!important;
    max-width:70%!important;
}
</style>
<h1 id="why-bayesian-classification">Why Bayesian Classification?</h1>
<hr>
<blockquote>
<ul>
<li>Decision tree classifiers predict the class label of y.
<ul>
<li>E.g., it is yes or no.</li>
</ul>
</li>
<li>Bayesian classifiers calculate probabilities for hypothesis.
<ul>
<li>E.g., the probabilities to be yes and no.</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><strong>Probabilistic learning</strong>: Calculate explicit probabilities for hypothesis, among the most practical approaches to certain types of learning problems.</li>
<li><strong>Incremental(递增的)</strong>: Each training example can incrementally increase/decrease the probability that a hypothesis is correct. Prior knowledge can be combined with observed data.</li>
<li><strong>Probabilistic prediction</strong>: Predict multiple hypotheses, weighted by their probabilities</li>
<li>Standard: Even when Bayesian methods are computationally intractable(难对付的), they can provide a standard of optimal decision making against which other methods can be measured</li>
</ul>
<h1 id="bayesian-theorem">Bayesian Theorem</h1>
<hr>
<ul>
<li>Let X be a data sample whose class label is <strong>unknown</strong></li>
<li>Let h be <strong>hypothesis</strong> that X belongs to class C</li>
<li>For classification problems, determine P(h|X): the probability that the hypothesis holds given the observed data sample X</li>
<li>P(h): <strong>prior</strong> probability of hypothesis h(i.e. the initial probability before we observe any data, reflects the background knowledge)</li>
<li>P(X): probability that sample data is observed</li>
<li>P(X|h): probability of observing the sample X, given that the hypothesis holds</li>
<li>Given training data X, posteriori probability of a hypothesis h, P(h|X) follows the Bayes theorem<br>
<img src="1.png" alt=""></li>
<li>Informally, this can be written as <strong>posterior = likelihood * prior / evidence</strong><br>
posterior: P(h|X)<br>
likelihood: P(X|h)<br>
prior: P(h)<br>
evidence: P(X)</li>
<li>MAP(<strong>maximum posteriori</strong>后期) hypothesis<br>
<img src="2.png" alt=""></li>
</ul>
<h2 id="naïve-bayes-classifier">Naïve Bayes Classifier</h2>
<ul>
<li>Assumption: attributes are <strong>conditionally independent</strong>:<br>
<img src="3.png" alt=""></li>
<li>The product of occurrence of say 2 elements x1 and x2,<br>
given the current class is C, is the product of the probabilities of each element taken separately, given the same class P([x1,x2],C) = P(x1,C) * P(x2,C)</li>
<li>Greatly reduces the computation cost, only count the class distribution -&gt; Only need to estimate P(x<sub>k</sub> | Ci)<br>
<br><br>
Example:<br>
<img src="4.png" alt=""><br>
Compute P(X|Ci) for each class<br>
X=(age&lt;=30 , income =medium, student=yes, credit_rating=fair)<br><br>
P(age=&quot;&lt;30&quot; | buys_computer=“yes”) = 2/9=0.222<br>
P(age=&quot;&lt;30&quot; | buys_computer=“no”) = 3/5 =0.6 <br><br>
P(income=“medium” | buys_computer=“yes”)= 4/9 =0.444<br>
P(income=“medium” | buys_computer=“no”) = 2/5 = 0.4 <br><br>
P(student=“yes” | buys_computer=&quot;yes)= 6/9 =0.667<br>
P(student=“yes” | buys_computer=“no”)= 1/5=0.2 <br><br>
P(credit_rating=“fair” | buys_computer=“yes”)=6/9=0.667<br>
P(credit_rating=“fair” | buys_computer=“no”)=2/5=0.4<br><br>
<strong>P(X|Ci)</strong> :<br>
P(X|buys_computer=“yes”)= 0.222 x 0.444 x 0.667 x 0.0.667 =0.044<br>
P(X|buys_computer=“no”)= 0.6 x 0.4 x 0.2 x 0.4 =0.019<br><br>
<strong>P(X|Ci)* P(Ci)</strong> :<br>
P(buys_computer=“yes”) = 9/14<br>
P(buys_computer=“no”) = 5/14<br><br>
P(X|buys_computer=“yes”) * P(buys_computer=“yes”)=0.028<br>
P(X|buys_computer=“no”) * P(buys_computer=“no”)=0.007<br>
<strong>So, X belongs to class “buys_computer=yes”</strong></li>
</ul>
<h2 id="smoothing">Smoothing</h2>
<ul>
<li>
<p>Pr[X<sub>i</sub> = v<sub>j</sub> | C<sub>k</sub>] could still be 0, <strong>if not observed in the training data</strong></p>
<ul>
<li>makes Pr[C<sub>k</sub> | X] = 0, regardless of other likelihood values of Pr[C<sub>t</sub> = v<sub>t</sub>| C<sub>k</sub>]</li>
</ul>
</li>
<li>
<p><strong>Add-1 Smoothing</strong></p>
<ul>
<li>reserve a <strong>small amount of probability for<br>
unseen probabilities</strong></li>
<li>(conditional) probabilities of observed events have to be adjusted to <strong>make the total probability equals 1.0</strong><br>
<img src="5.png" alt=""></li>
</ul>
</li>
<li>
<p>What’s the right value for B?</p>
<ul>
<li>B is used to make sure the total probability equals 1, ∑v<sub>j</sub> Pr[X<sub>i</sub>  =v<sub>j</sub>  |C<sub>k</sub> ]=1</li>
<li>B = dom(X<sub>i</sub>), i.e., # of values X<sub>i</sub> can take.  -&gt;<strong>how many distinct values that X have</strong><br>
<br><br>
Example: Consider no instance of age&lt;30 in the class<br>
<img src="6.png" alt=""><br>
X=(age&lt;=30 , income =medium, student=yes, credit_rating=fair)<br><br>
P(age=&quot;&lt;30&quot; | buys_computer=“no”) = (0 +1) / (5 +3) = 0.125<br>
P(income=“medium” | buys_computer=“no”) = (2 +1) / (5 +3) = 0.375<br>
P(student=“yes” | buys_computer=“no”)= (1 +1) / (5 +2) = 0.286<br>
P(credit_rating=“fair” | buys_computer=“no”)= (2 +1) / (5 +2) = 0.429<br>
<strong>P(X|Ci)</strong> : P(X|buys_computer=“no”)= 0.125 x 0.375 x 0.286 x 0.429 = 0.00575<br>
<strong>P(X|Ci)*P(Ci)</strong> : P(X|buys_computer=“no”) * P(buys_computer=“no”) = 0.00205</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>Probabilities</th>
<th>Without Smoothing</th>
<th>With Smoothing</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pr[&lt;=30 | No]</td>
<td>0 / 5</td>
<td>1 / 8</td>
</tr>
<tr>
<td>Pr[30…40 | No]</td>
<td>3 / 5</td>
<td>4 / 8</td>
</tr>
<tr>
<td>Pr[&gt;=40 | No]</td>
<td>2 / 5</td>
<td>3 / 8</td>
</tr>
</tbody>
</table>
<p><strong>不是只有probability=0的+1，所有的都+1</strong></p>
<h2 id="how-to-handle-numeric-values">How to Handle Numeric Values</h2>
<ul>
<li>Need to model the distribution of Pr[X<sub>i</sub> | C<sub>k</sub>]</li>
<li>Method 1: Gaussian Naïve Bayes<img src="7.png" alt=""></li>
<li>Method 2: Use binning to discretize the feature values</li>
</ul>
<h1 id="text-classification">Text Classification</h1>
<p>Naïve Bayes has been widely used in text classificiation (aka., text categorization)</p>
<ul>
<li>Document Classification<br>
Assign labels to each document or web-page:
<ul>
<li>Labels are most often topics such as Yahoo-categories<br>
e.g., “finance,” “sports,” “news&gt;world&gt;asia&gt;business”</li>
<li>Labels may be genres<br>
e.g., “editorials” “movie-reviews” “news”</li>
<li>Labels may be opinion on a person/product<br>
e.g.,“like”,“hate”,“neutral”</li>
<li>Labels may be domain-specific<br>
e.g., “interesting-to-me” : &quot;not-interesting-to-me&quot;<br>
e.g., contains adult language:&quot;doesn’t&quot;<br>
e.g., language identification: English, French, Chinese, …<br>
e.g., search vertical: about Linux versus not<br>
e.g.,“link spam”,“not link spam”</li>
</ul>
</li>
</ul>
<h2 id="basic-method">Basic Method</h2>
<ul>
<li>Attributes are text positions, values are words.<br>
<img src="12.png" alt=""></li>
<li>Assume the classification is independent of the<br>
positions of the words
<ul>
<li>Use same parameters for each position</li>
<li>Result is bag of words model</li>
</ul>
</li>
</ul>
<h3 id="learning">Learning</h3>
<ul>
<li>From training corpus, extract V = Vocabulary</li>
<li>Calculate required P(c<sub>j</sub>) and P(x<sub>k</sub>| c<sub>j</sub>) terms
<ul>
<li>For each c<sub>j</sub> in C do
<ul>
<li>docs<sub>j</sub> &lt;- subset of documents for which the target class is c<sub>j</sub></li>
<li><img src="10.png" alt=""></li>
</ul>
</li>
<li>Text<sub>j</sub> &lt;- single document containing all docs<sub>j</sub></li>
<li>for each word x<sub>k</sub> in Vocabulary
<ul>
<li>n<sub>k</sub> &lt;- number of occurrences of x<sub>k</sub> in Text<sub>j</sub><br>
<img src="11.png" alt=""><br>
<strong>α : smoothing</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="classifying">Classifying</h3>
<ul>
<li>positions &lt;- all word positions in current document which contain tokens found in Vocabulary</li>
<li>Return c<sub>NB</sub>, where<img src="12.png" alt=""></li>
</ul>
<h3 id="time-complexity">Time Complexity</h3>
<ul>
<li>
<p><strong>Training Time</strong>: O(|D|L<sub>d</sub> + |C||V|))<br>
where L<sub>d</sub> is the average length of a document in D.</p>
<ul>
<li>Assumes V and all D<sub>i</sub> , n<sub>i</sub>, and n<sub>ij</sub> pre-computed in O(|D|L<sub>d</sub>)<br>
time during one pass through all of the data.</li>
<li>Generally just <strong>O(|D|L<sub>d</sub>)</strong> since usually |C||V| &lt; |D|L<sub>d</sub></li>
</ul>
</li>
<li>
<p><strong>Test Time</strong>: O(|C| L<sub>t</sub>)<br>
where Lt is the average length of a test document.</p>
<ul>
<li>Very efficient overall, linearly proportional to the time needed to just read in all the data.</li>
</ul>
</li>
</ul>
<h2 id="multinomial多项式-model">Multinomial(多项式)  Model</h2>
<blockquote>
<p>sklearn.naive_bayes.MultinomialNB</p>
</blockquote>
<ul>
<li>Based on a statistical language model, turns out to be the <strong>bag of words</strong> model if using unigrams(一元)</li>
</ul>
<h3 id="underflow-prevention-log-space">Underflow Prevention: log space</h3>
<ul>
<li><strong>Multiplying lots of probabilities, which are between 0 and 1 by definition, can result in floating-point underflow.</strong></li>
<li>Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities.</li>
<li>Class with highest final un-normalized log probability score is still the most probable.<br>
<img src="13.png" alt=""></li>
<li>Note that model is now just max of sum of weights…</li>
</ul>
<h3 id="unigram-and-higher-order-language-models-in-information-retrieval">Unigram and higher-order Language models in Information Retrieval</h3>
<ul>
<li>
<p>P(“abcd”)= P(a) * P(b | a) * P(c | a b) * P(d | a b c)</p>
</li>
<li>
<p>Unigram(一元) model: 0-th order Markov Model</p>
<ul>
<li><strong>the probability of a word only depend on itself</strong></li>
<li>P(d | a b c) = P(d)</li>
</ul>
</li>
<li>
<p>Bigram(二元) Language Models: 1st order Markov Model</p>
<ul>
<li><strong>the probability of a word only depend on its previous token</strong></li>
<li>P(d | a b c) = P(d | c)</li>
<li>P(a b c d) = P(a) * P(b | a) * P(c | b) * P(d | c)</li>
</ul>
</li>
<li>
<p>The same with class-conditional probabilities,</p>
<ul>
<li>i.e., the unigram model<br>
P(“a b c d” | C) = P (a | C) * P ( b | C) * P (c | C) * P (d | C)</li>
</ul>
</li>
</ul>
<p><img src="8.png" alt=""></p>
<ul>
<li>
<p>Essentially, the classification is independent of the<br>
positions of the words</p>
<ul>
<li>Use same parameters for each position</li>
<li>Result is bag of words model <img src="9.png" alt=""><br>
E.g. “to be or not to be”</li>
</ul>
<table>
<thead>
<tr>
<th>x<sub>i</sub></th>
<th>To</th>
<th>Be</th>
<th>Or</th>
<th>Not</th>
</tr>
</thead>
<tbody>
<tr>
<td>θ<sub>i</sub></td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
</li>
</ul>
<h2 id="bernoulli伯努利-model">Bernoulli(伯努利) Model</h2>
<blockquote>
<p>sklearn.naive_bayes.BernoulliNB<br>
View a text as a <strong>set of tokens</strong>, a boolean vector in {0, 1}<sup>|V|</sup>, where V is the vocabulary.</p>
</blockquote>
<ul>
<li>V={a,b,c,d,e} = {x1,x2,x3,x4,x5}</li>
<li>Feature functions fi(text) = if text contains x<sub>i</sub></li>
<li>The feature functions extract a vector of {0, 1}<sup>|V| </sup>from any text<br>
E.g.,“a b c d”, “d e b”</li>
</ul>
<table>
<thead>
<tr>
<th>a</th>
<th>b</th>
<th>c</th>
<th>d</th>
<th>e</th>
<th>C</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>+</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>-</td>
</tr>
</tbody>
</table>
<h2 id="two-models-compare">Two Models Compare</h2>
<h3 id="naïve-bayes-assumption">Naïve Bayes assumption</h3>
<ul>
<li>Multinomial Naïve Bayes
<ul>
<li>One feature Xi for each word pos in document
<ul>
<li>feature’s values are all words in dictionary</li>
</ul>
</li>
<li><strong>Value of X<sub>i</sub> is the word in position i</strong></li>
<li>Naïve Bayes assumption:
<ul>
<li>Given the document’stopic, word in one position in the document tells us nothing about words in other positions</li>
</ul>
</li>
<li>Second assumption:
<ul>
<li><strong>Word appearance does not depend on position</strong> P(Xi =w|c)=P(Xj =w|c)<br>
for all positions i,j, word w, and class c</li>
<li>Just have one multinomial feature predicting all words<br>
E.g., “to be or not to be” = “to to be be or not”</li>
</ul>
</li>
</ul>
</li>
<li>Multivariate(多变量) Bernoulli(伯努利)
<ul>
<li>One feature Xw for each word in dictionary</li>
<li><strong>X<sub>w</sub> = true in document d if w appears in d</strong></li>
<li>Naive Bayes assumption:
<ul>
<li>Given the document’s topic, <strong>appearance of one word in the document tells us nothing about chances that another word appears</strong></li>
</ul>
</li>
<li>This is the model used in the <strong>binary independence model</strong> in classic probabilistic relevance feedback in hand-classified data (Maron in IR was a very early user of NB)</li>
</ul>
</li>
</ul>
<h3 id="parameter-estimation">Parameter estimation</h3>
<ul>
<li>Multinomial model:<br>
P(X<sub>i</sub> = w|c<sub>j</sub>)= fraction of times in which word w appears across all documents of topic c<sub>j</sub><br>
<br></li>
<li>Multivariate Bernoulli model:<br>
P(X<sub>w</sub> = t|c<sub>j</sub>)= fraction of documents of topic c<sub>j</sub> in which word w appears</li>
</ul>
<h3 id="classification">Classification</h3>
<p>Multinomial model is almost always more effective in text applications!</p>
<h3 id="feature-selection">Feature selection</h3>
<ul>
<li>Multinomial model:
<ul>
<li>means something different for multinomial NB. It means dictionary truncation(缩短)</li>
<li>The multinomial NB model only has 1 feature</li>
</ul>
</li>
<li>Multivariate Bernoulli model:<br>
In general feature selection is necessary for multivariate Bernoulli NB. Otherwise you suffer from noise, multi-counting.</li>
</ul>
<h2 id="naïve-bayesian-classifier-comments">Naïve Bayesian Classifier: Comments</h2>
<ul>
<li>Advantages:
<ul>
<li>Easy to implement</li>
<li>Good results obtained in most of the cases</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Assumption: class conditional independence , therefore loss of accuracy</li>
<li>Practically, dependencies exist among variables</li>
<li>E.g., hospitals: patients: Profile: age, family history etc<br>
Symptoms: fever, cough etc., Disease: lung cancer, diabetes etc</li>
<li>Dependencies among these cannot be modeled by Naïve Bayesian Classifier</li>
</ul>
</li>
<li>Better methods?
<ul>
<li>Bayesian Belief Networks</li>
<li>Logistic regression / maxent</li>
</ul>
</li>
</ul>

                

                <hr>
                <!-- Pager -->
                <ul class="pager">
                    
                        <li class="previous">
                            <a href="/2018/06/13/COMP-9321-Data-Services-Engineering-Review/" data-toggle="tooltip" data-placement="top" title="COMP9321 Data Services Engineering Review">&larr; Previous Post</a>
                        </li>
                    
                    
                        <li class="next">
                            <a href="/2018/06/08/Decision-Tree-Classifier/" data-toggle="tooltip" data-placement="top" title="Decision Tree Classifier">Next Post &rarr;</a>
                        </li>
                    
                </ul>

                <!-- duoshuo Share start -->
                
                <!-- 多说 Share end-->

                <!-- 多说评论框 start -->
                
                <!-- 多说评论框 end -->

                <!-- disqus comment start -->
                
                    <div class="comment">
                        <div id="disqus_thread" class="disqus-thread"></div>
                    </div>
                
                <!-- disqus comment end -->
            </div>
            
            <!-- Tabe of Content -->
            <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">Contents</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#why-bayesian-classification"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Why Bayesian Classification?</span></a></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#bayesian-theorem"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Bayesian Theorem</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#naïve-bayes-classifier"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">Naïve Bayes Classifier</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#smoothing"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">Smoothing</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#how-to-handle-numeric-values"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">How to Handle Numeric Values</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#text-classification"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Text Classification</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#basic-method"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Basic Method</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#learning"><span class="toc-nav-number">3.1.1.</span> <span class="toc-nav-text">Learning</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#classifying"><span class="toc-nav-number">3.1.2.</span> <span class="toc-nav-text">Classifying</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#time-complexity"><span class="toc-nav-number">3.1.3.</span> <span class="toc-nav-text">Time Complexity</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#multinomial多项式-model"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">Multinomial(多项式)  Model</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#underflow-prevention-log-space"><span class="toc-nav-number">3.2.1.</span> <span class="toc-nav-text">Underflow Prevention: log space</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#unigram-and-higher-order-language-models-in-information-retrieval"><span class="toc-nav-number">3.2.2.</span> <span class="toc-nav-text">Unigram and higher-order Language models in Information Retrieval</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#bernoulli伯努利-model"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">Bernoulli(伯努利) Model</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#two-models-compare"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">Two Models Compare</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#naïve-bayes-assumption"><span class="toc-nav-number">3.4.1.</span> <span class="toc-nav-text">Naïve Bayes assumption</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#parameter-estimation"><span class="toc-nav-number">3.4.2.</span> <span class="toc-nav-text">Parameter estimation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#classification"><span class="toc-nav-number">3.4.3.</span> <span class="toc-nav-text">Classification</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#feature-selection"><span class="toc-nav-number">3.4.4.</span> <span class="toc-nav-text">Feature selection</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#naïve-bayesian-classifier-comments"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">Naïve Bayesian Classifier: Comments</span></a></li></ol></li></ol>
        
        </div>
      </aside>
    

                
            <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5><a href="/tags/">FEATURED TAGS</a></h5>
                    <div class="tags">
                       
                          <a class="tag" href="/tags/#COMP9318" title="COMP9318">COMP9318</a>
                        
                          <a class="tag" href="/tags/#Data Mining" title="Data Mining">Data Mining</a>
                        
                          <a class="tag" href="/tags/#Classification" title="Classification">Classification</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
            </div>
        </div>
    </div>
</article>




<!-- disqus embedded js code start (one page only need to embed once) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "jingxuan-li";
    var disqus_identifier = "https://jingxuan.li/2018/06/08/Bayesian-Classifiers/";
    var disqus_url = "https://jingxuan.li/2018/06/08/Bayesian-Classifiers/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus embedded js code start end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'left',
          icon: 'ℬ'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">
                
                
                

                

                
                    <li>
                        <a target="_blank" href="https://www.facebook.com/jingxuan0">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-facebook fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://github.com/jaceyli">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                
                    <li>
                        <a target="_blank"  href="https://www.linkedin.com/in/jingxuan-li-490a60152/">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Jingxuan 2018 
                    <br>
                    Theme by <a href="http://huangxuan.me">Hux</a> 
                    <span style="display: inline-block; margin: 0 5px;">
                    | 
                    <span id="busuanzi_container_site_pv">
                     Total <span id="busuanzi_value_site_pv"></span> Hits
                    </span>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js"></script>

<!-- 不蒜子统计 -->
<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- 
     Because of the native support for backtick-style fenced code blocks 
     right within the Markdown is landed in Github Pages, 
     From V1.6, There is no need for Highlight.js, 
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0  
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/    
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->


<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("https://jingxuan.li/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>

<!-- Google Analytics -->


<script>
    // dynamic User by Hux
    var _gaId = 'UA-XXXXXXXX-X';
    var _gaDomain = 'yoursite';

    // Originial
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', _gaId, _gaDomain);
    ga('send', 'pageview');
</script>




<!-- Baidu Tongji -->






	<a id="rocket" href="#top" class=""></a>
	<script type="text/javascript" src="/js/totop.js?v=1.0.0" async=""></script>
    <script type="text/javascript" src="/js/toc.js?v=1.0.0" async=""></script>


</body>

</html>
